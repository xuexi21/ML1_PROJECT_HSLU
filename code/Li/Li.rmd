# Machine Learning Project

## Team memebers

| Name                | Email                                                                      |
|:----------------------|:--------------------------------------------------|
| Xuekai Li           | [xuekai.li\@stud.hslu.ch](mailto:xuekai.li@stud.hslu.ch){.email}           |
| Dmytro Rudyka       | [dmytro.rudyka\@stud.hslu.ch](mailto:dmytro.rudyka@stud.hslu.ch){.email}   |
| Sasa Ljubisavljevic | [sasa.ljubisavljevic\@stud.hslu.ch](mailto:sasa.ljubisavljevic@stud.hslu.ch){.email} |

## 1.Topic : Health Insurance Data Prediction
`
### 1.1 Problem Statement

1.  Prediction of continuous data: : A key challenge for the insurance industry is to charge each customer an appropriate premium for the risk they represent. The ability to predict a correct claim amount has a significant impact on insurer's management decisions and financial statements. Predicting the cost of claims in an insurance company is a real-life problem that needs to be solved in a more accurate and automated way. Several factors determine the cost of claims based on health factors like BMI, age, smoker, health conditions and others. Insurance companies apply numerous techniques for analyzing and predicting health insurance costs.

2.  Prediction of binary data: A critical challenge for the insurance industry is accurately determining whether a customer is a smoker or a non-smoker. This information plays a crucial role in assessing the risk profile of individuals and calculating appropriate premiums.

3.  Prediction of count data:: Prediction of count data: In addition to predicting binary data like smoking status, insurance companies face the task of estimating the number of dependents an individual has. This count data is crucial for assessing the potential financial liabilities associated with providing coverage for dependents. Accurate prediction of the number of dependents helps insurers determine appropriate coverage limits, calculate premiums, and manage their resources efficiently.

### 1.2 Data Definition

data source: <https://www.kaggle.com/datasets/sureshgupta/health-insurance-data-set>

observation: 15000 rows

predictors: 13 columns

| Dimension           | Explanation                                                                                                                                                                                                 |
|:------------------------------------|:------------------------------------|
| age                 | Age of the policyholder (Numeric)                                                                                                                                                                           |
| sex                 | Gender of policyholder (Categoric)                                                                                                                                                                          |
| weight              | Weight of the policyholder (Numeric)                                                                                                                                                                        |
| bmi                 | Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m \^ 2) using the ratio of height to weight (Numeric) |
| no_of_dependents    | Number of dependent persons on the policyholder (Numeric)                                                                                                                                                   |
| smoker              | Indicates policyholder is a smoker or a non-smoker (non-smoker=0;smoker=1) (Categoric)                                                                                                                      |
| claim               | The amount claimed by the policyholder (Numeric)                                                                                                                                                            |
| bloodpressure       | Bloodpressure reading of policyholder (Numeric)                                                                                                                                                             |
| diabetes            | Indicates policyholder suffers from diabetes or not (non-diabetic=0; diabetic=1) (Categoric)                                                                                                                |
| regular_ex          | A policyholder regularly excercises or not (no-excercise=0; excercise=1) (Categoric)                                                                                                                        |
| job_title           | Job profile of the policyholder (Categoric)                                                                                                                                                                 |
| city                | The city in which the policyholder resides (Categoric)                                                                                                                                                      |
| hereditary_diseases | A policyholder suffering from a hereditary diseases or not (Categoric)                                                                                                                                      |

## 2 cleaning and prepare the data

**Load the packages and the dataset**

```{r}
# load the packages
library(tidyverse)              # load, clean, visualize the data.
theme_set(theme_bw())           # set the theme for GGPLOT2

# load the packages
df <- read_csv("healthinsurance.csv")

# check the structure
str(df)

# check the NA 
summary(df)
```

From the above results, we can determine that our dataset consists of 15,000 rows of records and 13 dimensions, which aligns with the dataset description. Upon closer inspection, we can identify the following characteristics:

There are 4 categorical variables: sex, hereditary diseases, city, and job title.\
There are 9 numerical variables: age, weight, BMI, number of dependents, smoker status, blood pressure, diabetes status, regular exercise, and claim.

Additionally, there are some missing values in the dataset:

There are 396 NAs in the Age column.\
There are 956 NAs in the BMI column.

##### cleaning - drop_na().

```{r}
# clean the data
df <- df %>% drop_na()

# check the correlation of the numerical data

pairs(select(df, age, weight, bmi, bloodpressure, claim),
      pch = ".",
      lower.panel = panel.smooth)

```

From the plot, we can see that the blood pressure has a value of 0, which is an outlier. Therefore, it should be replaced with the mean value.

```{r}
# Replace 0 values in 'bloodpressure' column with mean of non-zero values
df$bloodpressure[df$bloodpressure == 0] <- mean(df$bloodpressure[df$bloodpressure != 0])
# now all the data have no outliers
# pairs(select(df,age,weight,bmi,bloodpressure,claim))
```

## 3 Visual Explore the data.

**find out the relationship of claim and other variables.**

first let us see the distribution of claim variable.

```{r}
# ust histogram plot to see the distruibution of claim
ggplot(df,aes(x = claim)) + geom_histogram(bins=50)


```

the plot shows a right skews disturibution, so we should better use log() function to transform the claim variable for the later analysing.

```{r}
# load the packages
library(ggpubr)

# boxplot the claim ~ sex
plot_sex <- ggplot(df, aes(sex, log(claim), fill=sex)) +
            geom_boxplot() 

# box plot the claim ~ smoker
plot_smoker <- ggplot(df, aes(as.factor(smoker), log(claim), fill=as.factor(smoker))) +
                # estimate the mean of NON smoker group
                geom_hline(yintercept=8.95,col="blue",linetype = "dashed") +
                # estimate the mean of smoker group
                geom_hline(yintercept=10.45,col="blue",linetype = "dashed") +
                geom_boxplot() 

ggarrange(plot_sex, plot_smoker + rremove("x.text"), 
          labels = c(" gender", "smoker"),
          ncol = 2, nrow = 1)


```

```{r}
# Calculate the difference in approximation
exp(10.45) - exp(8.95)
```

Based on the above boxplot analysis, we have made the following observations:

1.  Gender does not appear to have a significant impact on the claim amount. The average claim is nearly the same for both males and females.
2.  On the other hand, being a smoker has a substantial effect on the claim amount. On average, smokers tend to have a claim amount that is approximately \$26840 higher than non-smokers.

```{r}
# claim ~ hereditary_diseases
table(df$hereditary_diseases)
ggplot(df, aes(hereditary_diseases, log(claim), fill=hereditary_diseases)) +
    geom_hline(yintercept=9.1) +
    geom_hline(yintercept=10.6) +
    geom_boxplot() +
    ggtitle('Boxplot of claims by hereditary_diseases') +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```

here i can transform the hereditary diseases data to binary data 0: no diseases and 1: have diseases. which can help for later analyse.

```{r}
# claim ~ have_diseases

df$have_diseases <- ifelse(df$hereditary_diseases == "NoDisease",0,1)
ggplot(df, aes(have_diseases, log(claim), fill=as.factor(have_diseases))) +
    # estimate the mean(claim) of have no diseases group
    geom_hline(yintercept=9.1,col="blue",linetype = "dashed") +
    # estimate the mean(claim) of have diseases group
    geom_hline(yintercept=10.6,col="blue",linetype = "dashed") +
    geom_boxplot()
# calculate the average claim amount of approximately for the Customers without any hereditary diseases. 
exp(9.1)
# calculate the average claim amount of approximately for the Customers have hereditary diseases. 
exp(10.6)
```

The box plot analysis reveals that hereditary diseases also have a significant impact on the claim amount. Customers without any hereditary diseases have an average claim amount of approximately \$8955. In contrast, customers with hereditary diseases have significantly higher average claim amounts ranging from \$40135.

```{r}
# claim ~ no_of_dependents
table(df$no_of_dependents)
ggplot(df, aes(as.factor(no_of_dependents), claim, fill=as.factor(no_of_dependents))) +
geom_boxplot() +
ggtitle('Boxplot of claims by no_of_dependents')
```

The box plot indicates that the number of dependents does not have a significant influence on the claim amount.
```{r}
# claim ~ diabetes
table(df$diabetes)
plot_diabetes <- ggplot() +
                    geom_boxplot(data =df, aes(x = as.factor(diabetes), y = claim, fill= as.factor(diabetes))) 

# claim ~ regular_ex
table(df$regular_ex)
plot_ex <- ggplot(df, aes(as.factor(regular_ex), claim,fill=as.factor(regular_ex))) +
            geom_boxplot()

ggarrange(plot_diabetes, plot_ex + rremove("x.text"), 
          labels = c("diabetes", "regular_ex"),
          ncol = 2, nrow = 1)


```

The above box plot clearly shows that customers with diabetes have significantly higher claim amounts compared to those without diabetes.

Additionally, regular exercise does not have a significant impact on claim amounts.
```{r}
# log(claim) ~ job_title
table(df$job_title)
ggplot(df, aes(job_title, log(claim), fill=job_title)) +
    geom_boxplot()  +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

The above box plot reveals that job also has a significant impact on claim amounts.  on average
Labour and students have the lower claim amount, other jobs have higher claim amount.

```{r}
# log(claim) ~ age
ggplot(data = df, mapping = aes(y = log(claim),x = age)) +
            geom_point() +
            geom_smooth()
```

The plot illustrates the relationship between the logarithm of claim amounts and age. The smooth term indicates a positive non-linear relationship, implying that the claim amount tends to increase with age. This suggests that fitting a general additive model later would be beneficial in capturing the intricate nature of this relationship.
```{r}
# log(claim) ~ bmi
ggplot(df, aes(bmi, log(claim))) +
            geom_point() +
            geom_smooth()
```

The plot illustrates the relationship between the logarithm of claim amounts and bmi. The smooth term indicates a positive non-linear relationship, implying that the claim amount tends to increase with bmi.
```{r}
# log(claim) ~ bloodpressure
ggplot(df, aes(bloodpressure, log(claim))) +
            geom_point( ) +
            geom_smooth()
```

The plot indicates that there is not a clear relationship between the logarithm of claim amounts and blood pressure. 
```{r}
# log(claim) ~ weight
ggplot(df, aes(weight, claim)) +
                geom_point() +
                geom_smooth()
```

Again, The plot indicates that there is a complex relationship between the logarithm of claim amounts and weight.

## 4. To fit the models and make predictions for the Target variable.
### 4.1 Claim (continuous data):

- Linear Regression: Fit a linear regression model to predict the claim amount based on relevant features. Analyze the coefficients of each variable to understand their impact on the claim amount.
- Generalized Additive Model (GAM) Regression: Use a GAM regression model to capture non-linear relationships between the predictors and the claim amount. Analyze the smooth terms to understand the shape of the relationships.
- Support Vector Machines (SVM) Regression: Apply SVM regression to predict the claim amount. This model can handle non-linear relationships and is suitable for continuous data.

**prepare the train and test dataset for Cross validation.**

```{r}
# load caret
library(caret)
df_transformed <- df %>% select(age, weight, bmi, have_diseases, no_of_dependents, smoker, bloodpressure, diabetes, regular_ex, claim, sex, city, job_title)
# prepare the data to train set and test set for cross validation.
set.seed(123)
indexes <- createDataPartition(df_transformed$claim, p=.85, list = F)
train <-  df_transformed[indexes, ]
test <-  df_transformed[-indexes, ]

```

#### linear model regression

First, let's train a linear model with all the predictors.

```{r}
set.seed(123)
# Train a linear model
lm.1 <- lm(data= train, log(claim) ~ .)

#  use drop1() to analyze the multi level catergory variables
drop1(lm.1,test = "F")
```

From the output, we can make the following observations:

- Dropping the "age," "sex," "bmi," "hereditary_diseases," "no_of_dependents," "smoker," "diabetes," and "job_title" variables from the model significantly affects the model fit, as indicated by the low p-values (<0.05) in the "Pr(>F)" column.
- The "weight," "city," "bloodpressure," and "regular_ex" variables, on the other hand, do not significantly impact the model fit, as their p-values are greater than 0.05.
```{r}
# keep the R^2 and RMSE measurement of lm.1

# get the R^2 of lm.1
r_sq_1 <- summary(lm.1)$r.squared

# use test dataset to get he prediction.
prediction_1 <- predict(lm.1, newdata = test)

# Calculate the residuals and RMSE.
residuals_1 <- log(test$claim) - prediction_1
rmse_1 <- sqrt(mean(residuals_1^2))


```


**update lm.1 model**\
Now, we will train a new model with a reduced set of predictors based on the drop1() analysis.

```{r}
lm.2 <- update(lm.1, ~. -city - weight - regular_ex - bloodpressure )
summary(lm.2)
#drop1(lm.2,test = "F")

# keep the R^2 and RMSE measurement of lm.2
# get the R^2 of lm.2
r_sq_2 <- summary(lm.2)$r.squared

# use test dataset to get he prediction.
prediction_2 <- predict(lm.2, newdata = test)

# Calculate the residuals and RMSE.
residuals_2 <- log(test$claim) - prediction_2
rmse_2 <- sqrt(mean(residuals_2^2))


```


Coefficients:\
The coefficients section displays the estimated coefficients for each predictor variable in the model. These coefficients indicate the expected change in the logarithm of the claim for a one-unit change in the corresponding predictor, while holding all other variables constant.

For example:
the coefficient for "age" is 0.0339, which means that for every one-unit increase in age, the logarithm of the claim is expected to increase by 0.0339 units, assuming all other variables remain constant.

**Compare the models**

```{r}
print(paste0("R-squared for first model:", round(r_sq_1, 4)))
print(paste0("R-squared for second model:", round(r_sq_2, 4)))
print(paste0("RMSE for first model: ", round(rmse_1, 3)))
print(paste0("RMSE for second model: ", round(rmse_2, 3)))
```

The lm.1 model fits 75.1% of the data, while the lm.2 model fits 74.9% of the data. However, since both models have the same root mean squared error, I will choose to keep the lm.2 model because it is simpler.

#### GAM model regression

```{r}
# check the collinearity of each variable
library(car)
vif(lm.1)

```

The GVIF (Generalised Variance Inflation Factor) value tell us how serious is the issue of collinearity.\
A value above 5 is usually regarded as “if possible, remove that variable”.\
A value above 10 means that the collinearity is too high and some action must be undertaken.

The output shows there have no collinearity issues. so we could continue use those variables.

Now, we can train a GAM model and apply smoothing to all continuous variables.
```{r}
# load the mgcv packages
library(mgcv)

## fit the model
gam.1 <- gam(log(claim) ~ s(age)+ s(weight) +s(bmi) +s(bloodpressure) + have_diseases + no_of_dependents + smoker + diabetes + regular_ex + sex  + city + job_title, data=train)
summary(gam.1)
```

The results show that when continuous variables such as age, BMI, weight, and blood pressure have higher degrees (around 7-8), they have a significant impact on the model fit. This is in contrast to the results of the linear model.\
**Now, let's assess the performance of the GAM.1 model and compare it to the LM.2 model.**
```{r}

# keep the R^2 and RMSE measurement of gam.1
# from above result we know the R^2 is 0.76
r_sq_3 <- 0.76

# use test dataset to get he prediction.
prediction_3 <- predict(gam.1, newdata = test)

# Calculate the residuals and RMSE.
residuals_3 <- log(test$claim) - prediction_3
rmse_3 <- sqrt(mean(residuals_3^2))


print(paste0("R-squared for third model:", round(r_sq_3, 4)))
print(paste0("R-squared for second model:", round(r_sq_2, 4)))
print(paste0("RMSE for third model: ", round(rmse_3, 3)))
print(paste0("RMSE for second model: ", round(rmse_2, 3)))
```

The GAM model demonstrates an R-squared value of 76%, which is higher than the lm.2 model's 74.9%. This indicates that the GAM model fits the data more effectively, with a larger proportion of the data being explained by the model. Additionally, the RMSE (Root Mean Squared Error) for the GAM model is 0.444, which is smaller than the lm.2 model's RMSE of 0.451. This suggests that the GAM model has a smaller average prediction error, further highlighting its superior performance compared to the lm.2 model.

#### SVM model regression
```{r}
# load the packages for SVM

library(e1071)
# SVM regression
svm.1 <- svm(log(claim) ~ ., data = train)            # log transform the data
summary(svm.1)
```


compare the SVM model with GAM model
```{r}
# use test dataset to get the prediction.
prediction_4 <- predict(svm.1, test)

# Calculate the residuals and RMSE.
residuals_4 <- log(test$claim) - prediction_4
rmse_4 <- sqrt(mean(residuals_4^2))

# 
print(paste0("RMSE for third model: ", round(rmse_3, 3)))
print(paste0("RMSE for fourth model: ", round(rmse_4, 3)))
```

The SVM model achieves an RMSE of 0.391. This indicates that the average prediction error of the SVM model is smaller compared to both the GAM and LM.2 models. The SVM model provides more accurate predictions for the given health insurance data. Therefore, based on the RMSE values alone, we can conclude that **the SVM model outperforms both the GAM and LM.2 models in terms of prediction accuracy.**


### 4.2 Smoker (binary data) classification problem:

- Logistic Regression: Fit a logistic regression model to predict the binary outcome of smoker or non-smoker based on relevant predictors. Analyze the coefficients to understand the impact of each variable on the likelihood of being a smoker.
- Support Vector Machines (SVM): Utilize SVM algorithms with appropriate kernel functions for binary classification. SVM can handle non-linear relationships and is suitable for binary classification problems.
- Neural Network Classification: Employ a neural network model to classify individuals as smokers or non-smokers. Neural networks can capture complex patterns and relationships in the data.

#### Logistic regression.

```{r}
# train the model
glm.smoker <- glm(data=train, smoker ~., family="binomial" )

# use test dataset to get he prediction.
prediction_5 <- predict(glm.smoker, test, type="response")

# use threshold 0.5 to classify the smoker
prediction_5 <- ifelse(prediction_5 > 0.5, 1, 0)

# check the result of the logistic model
confusionMatrix(as.factor(test$smoker),as.factor(prediction_5))

```

After fitting the logistic model using the training dataset, we can utilize the model to make predictions on the test dataset. By comparing these predictions with the actual values in the test dataset, we can evaluate the model's performance using a confusion matrix. We get the accuracy is 94.33%
```{r}



```

```{r}



```


```{r}



```



```{r}



```
### 4.3 Number of dependents (count data):

- Generalized Linear Model (GLM) Poisson Regression: Fit a Poisson regression model to predict the count of dependents based on relevant features. This model is appropriate for count data.

```{r}



```
